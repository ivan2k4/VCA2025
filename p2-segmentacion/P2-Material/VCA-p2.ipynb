{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "creative-mistake",
   "metadata": {
    "id": "creative-mistake"
   },
   "source": [
    "# GIA - Práctica 2\n",
    "## Segmentación\n",
    "\n",
    "El objetivo de esta práctica es el desarrollo de una metodología para la segmentación automática de las regiones de fluido patológico en imágenes OCT. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GWpow_9AqASA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1631,
     "status": "ok",
     "timestamp": 1679998148680,
     "user": {
      "displayName": "Lucía Ramos García",
      "userId": "01153031010548417511"
     },
     "user_tz": -120
    },
    "id": "GWpow_9AqASA",
    "outputId": "37034543-7944-4d2d-d59e-4bc6fb7f9b76"
   },
   "outputs": [],
   "source": [
    "# Connect to drive\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "route = '/content/gdrive/MyDrive/GIA/PVC/P2/'\n",
    "\n",
    "print('\\nChange the directory to project route')\n",
    "%cd $route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-positive",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1679998154030,
     "user": {
      "displayName": "Lucía Ramos García",
      "userId": "01153031010548417511"
     },
     "user_tz": -120
    },
    "id": "abandoned-positive",
    "outputId": "d155ad33-19a8-405d-b5be-63513d665dba"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aux function that shows an image and its corresponding mask in the same figure\n",
    "def show(image, mask, title=None):\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].axis('off')\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    ax[1].imshow(mask, cmap=\"gray\")\n",
    "    ax[1].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "im = plt.imread(f\"{route}/OCT-dataset/images/sample_01.jpg\")\n",
    "mask = plt.imread(f\"{route}/OCT-dataset/masks/sample_01.jpg\")\n",
    "show(im,mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95002c8f",
   "metadata": {
    "id": "95002c8f"
   },
   "source": [
    "**Sugerencia**: Para el cálculo de métricas de rendimiento se puede utilizar la siguiente función auxiliar que permite obtener la máscara de segmentación a partir de las salidas de la red. La salida de la red es un valor positivo en coma flotante. La función sigmoidea restringe el dominio de salida entre 0 y 1. Como la máscara de salida es binaria, usamos un umbral para establecer los valores de la máscara de salida en 0 ó 1. Ten en cuenta que tal vez tengas que ajustar el umbral por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756044d",
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1679998362272,
     "user": {
      "displayName": "Lucía Ramos García",
      "userId": "01153031010548417511"
     },
     "user_tz": -120
    },
    "id": "4756044d"
   },
   "outputs": [],
   "source": [
    "# Aux function to get binary segmentation mask\n",
    "import torch\n",
    "def get_segmentation_masks(outputs, threshold=0.5):\n",
    "    probs = torch.sigmoid(outputs)\n",
    "    masks = (probs > threshold)*1.0\n",
    "    return masks\n",
    "\n",
    "# Aux function to show results\n",
    "def show_result(orig, gt, prediction, title=None):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    ax = axes.ravel()\n",
    "\n",
    "    images = [orig, gt, prediction, orig*prediction]\n",
    "    titles = ['Orig', 'Gt', 'Result', 'Overlap']\n",
    "    for i, (im, tit) in enumerate(zip(images, titles)):\n",
    "        ax[i].imshow(im, cmap='gray')\n",
    "        ax[i].set_title(tit)\n",
    "        ax[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-timber",
   "metadata": {
    "id": "meaningful-timber"
   },
   "source": [
    "## UNet\n",
    "UNet es una red convolucional diseñada para la segmentación de imágenes biomédicas. Sus tasas de éxito son altas incluso con pocas imágenes en el conjunto de entrenamiento. \n",
    "En esta práctica abordaremos la tarea de segmentación usando una UNet. Los pasos generales a seguir son los siguientes:\n",
    "1. Definir la arquitectura de red\n",
    "2. Creación de dataset\n",
    "   - Generar conjuntos de entrenamiento, validación y test\n",
    "3. Entrenar la red\n",
    "4. Seleccionar modelo en base a conjunto de validación\n",
    "5. Test del modelo seleccionado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-polymer",
   "metadata": {
    "id": "future-polymer"
   },
   "source": [
    "### Paso 1: Arquitectura de red\n",
    "\n",
    "Puedes implementar la red desde cero a partir de la descripción proporcionada en el [paper original](https://arxiv.org/abs/1505.04597), utilizar algún paquete de python que proporciona la implementación y modelos preentrenados como [Segmentation Models based on PyTorch](https://segmentation-modelspytorch.readthedocs.io/en/latest/) o utilizar el código fuente disponible online de alguna de las arquitecturas de red típicas. Por ejemplo, la siguiente implementación adaptada [de este código fuente](https://github.com/usuyama/pytorch-unet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-butter",
   "metadata": {
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1679998169503,
     "user": {
      "displayName": "Lucía Ramos García",
      "userId": "01153031010548417511"
     },
     "user_tz": -120
    },
    "id": "realistic-butter"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )   \n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, n_class):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(input_channels, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)        \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "        \n",
    "        x = self.dconv_down4(x)\n",
    "        \n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        \n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        \n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-navigator",
   "metadata": {
    "id": "medieval-navigator"
   },
   "source": [
    "### Paso 2: Creación de Dataset\n",
    "En este paso, vamos a cargar los datos extendiendo la clase `Dataset` de torch. \n",
    "\n",
    "Dado que el número de ejemplos en nuestro conjunto de datos es pequeño, podemos aplicar algunas transformaciones (rotaciones, traslaciones, suavizado, ...) para generar muestras artificiales y, de esta manera, aumentar el número de muestras en nuestro conjunto de datos. Esto se denomina aumento de datos. Puedes encontrar más información sobre el aumento de datos y su implementación en las [páginas de documentación de PyTorch](https://pytorch.org/vision/stable/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-ecuador",
   "metadata": {
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1679998177355,
     "user": {
      "displayName": "Lucía Ramos García",
      "userId": "01153031010548417511"
     },
     "user_tz": -120
    },
    "id": "personalized-ecuador"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "class OCTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_path, mask_path, rsize = (416,624), transform = None):\n",
    "        super().__init__()\n",
    "        # Load all the filenames with extension tif from the image_path directory\n",
    "        self.img_files = glob.glob(os.path.join(image_path,'*.jpg'))\n",
    "\n",
    "        self.mask_files = []\n",
    "\n",
    "        # We asume that each image has the same filename as its corresponding mask\n",
    "        # but it is stored in another directory (mask_path)\n",
    "        for img_path in self.img_files:\n",
    "             self.mask_files.append(os.path.join(mask_path, os.path.basename(img_path)))\n",
    "                \n",
    "        self.rsize = rsize  # Size to use in default Resize transform\n",
    "        self.transform = transform\n",
    "\n",
    "    # Returns both the image and the mask\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_files[index]\n",
    "        mask_path = self.mask_files[index]\n",
    "        image = plt.imread(img_path)\n",
    "        mask = plt.imread(mask_path)\n",
    "        if len(mask.shape) > 2:\n",
    "            mask = mask[:,:,0]\n",
    "        if len(image.shape) > 2:\n",
    "            image = image[:,:,0]\n",
    "        _, mask = cv2.threshold(mask, 100, 255, cv2.THRESH_BINARY) # Make sure that mask is binary\n",
    "        # Apply the defined transformations to both image and mask\n",
    "        if self.transform is not None:\n",
    "            seed = np.random.randint(2147483647) # make a seed with numpy generator \n",
    "            random.seed(seed) # apply this seed to image transforms\n",
    "            torch.manual_seed(seed) \n",
    "            image = self.transform(image)\n",
    "            random.seed(seed) # apply the same seed to mask transforms\n",
    "            torch.manual_seed(seed) \n",
    "            mask = self.transform(mask)\n",
    "        else:\n",
    "            t = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(self.rsize, interpolation= InterpolationMode.NEAREST),\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "            image = t(image)\n",
    "            mask = t(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-newspaper",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 1195,
     "status": "ok",
     "timestamp": 1679998182106,
     "user": {
      "displayName": "Lucía Ramos García",
      "userId": "01153031010548417511"
     },
     "user_tz": -120
    },
    "id": "reduced-newspaper",
    "outputId": "41d7531f-f411-4ae8-bc57-feda46c0f692"
   },
   "outputs": [],
   "source": [
    "# Load some samples\n",
    "simple_dataset = OCTDataset(f\"{route}/OCT-dataset/images\", f\"{route}/OCT-dataset/masks\")\n",
    "print(\"Dataset len:\", len(simple_dataset))\n",
    "nsamples = 4\n",
    "for _ in range(nsamples):\n",
    "    idx = np.random.randint(0, len(simple_dataset))\n",
    "    im, mask = simple_dataset[idx]\n",
    "    show(im.squeeze(), mask.squeeze(), title=f\"Sample {idx}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
